{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('numba').setLevel(logging.WARNING)\n",
    "%matplotlib inline\n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "import commons\n",
    "import utils\n",
    "import ONNXVITS_infer\n",
    "from text import text_to_sequence\n",
    "\n",
    "def get_text(text, hps):\n",
    "    text_norm = text_to_sequence(text, hps.symbols, hps.data.text_cleaners)\n",
    "    if hps.data.add_blank:\n",
    "        text_norm = commons.intersperse(text_norm, 0)\n",
    "    text_norm = torch.LongTensor(text_norm)\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.mirrors.ustc.edu.cn/simple\r\n",
      "Requirement already satisfied: matplotlib in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (9.4.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (1.0.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nano/.local/lib/python3.10/site-packages (from matplotlib) (23.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (4.39.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (1.23.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/nano/miniconda3/envs/vits/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = utils.get_hparams_from_file(\"model/1374_epochs.pth.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_g = ONNXVITS_infer.SynthesizerTrn(\n",
    "    len(hps.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    n_speakers=hps.data.n_speakers,\n",
    "    **hps.model)\n",
    "_ = net_g.eval()\n",
    "\n",
    "_ = utils.load_checkpoint(\"model/1374_epochs.pth\", net_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model requires 4 inputs. Input Feed contains 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m     x_tst_lengths \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor([stn_tst\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)])\n\u001B[1;32m      6\u001B[0m     sid \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor([\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m----> 7\u001B[0m     audio \u001B[38;5;241m=\u001B[39m \u001B[43mnet_g\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_tst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_tst_lengths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m.667\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_scale_w\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlength_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mfloat()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m      8\u001B[0m ipd\u001B[38;5;241m.\u001B[39mdisplay(ipd\u001B[38;5;241m.\u001B[39mAudio(audio, rate\u001B[38;5;241m=\u001B[39mhps\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39msampling_rate, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "File \u001B[0;32m~/Project/PycharmProjects/VitsServer/ONNXVITS_infer.py:74\u001B[0m, in \u001B[0;36mSynthesizerTrn.infer\u001B[0;34m(self, x, x_lengths, sid, noise_scale, length_scale, noise_scale_w, max_len, emotion_embedding)\u001B[0m\n\u001B[1;32m     72\u001B[0m zinput \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mrandn(x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m2\u001B[39m, x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m2\u001B[39m))\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype) \u001B[38;5;241m*\u001B[39m noise_scale_w)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sid \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     g \u001B[38;5;241m=\u001B[39m \u001B[43mrunonnx\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mONNX_net/dp.onnx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msid\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     g \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(g)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     76\u001B[0m     logw \u001B[38;5;241m=\u001B[39m runonnx(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mONNX_net/dp.onnx\u001B[39m\u001B[38;5;124m\"\u001B[39m, x\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mnumpy(), x_mask\u001B[38;5;241m=\u001B[39mx_mask\u001B[38;5;241m.\u001B[39mnumpy(), zin\u001B[38;5;241m=\u001B[39mzinput\u001B[38;5;241m.\u001B[39mnumpy(), g\u001B[38;5;241m=\u001B[39mg\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "File \u001B[0;32m~/Project/PycharmProjects/VitsServer/ONNXVITS_utils.py:35\u001B[0m, in \u001B[0;36mrunonnx\u001B[0;34m(model_path, x, x_lengths, sid, x_mask, zin, g, y_mask, z_p)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m z_p \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     33\u001B[0m     input_feed[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mz_p\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m z_p\n\u001B[0;32m---> 35\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43msess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_feed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/miniconda3/envs/vits/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:196\u001B[0m, in \u001B[0;36mSession.run\u001B[0;34m(self, output_names, input_feed, run_options)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# the graph may have optional inputs used to override initializers. allow for that.\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_inputs \u001B[38;5;241m<\u001B[39m num_required_inputs:\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel requires \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m inputs. Input Feed contains \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(num_required_inputs, num_inputs))\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m output_names:\n\u001B[1;32m    198\u001B[0m     output_names \u001B[38;5;241m=\u001B[39m [output\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_meta]\n",
      "\u001B[0;31mValueError\u001B[0m: Model requires 4 inputs. Input Feed contains 1"
     ]
    }
   ],
   "source": [
    "text1 = get_text(\"おはようございます。\", hps)\n",
    "stn_tst = text1\n",
    "with torch.no_grad():\n",
    "    x_tst = stn_tst.unsqueeze(0)\n",
    "    x_tst_lengths = torch.LongTensor([stn_tst.size(0)])\n",
    "    sid = torch.LongTensor([0])\n",
    "    audio = net_g.infer(x_tst, x_tst_lengths, sid=sid, noise_scale=.667, noise_scale_w=0.8, length_scale=1)[0][0,0].data.cpu().float().numpy()\n",
    "ipd.display(ipd.Audio(audio, rate=hps.data.sampling_rate, normalize=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tacotron2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8aad0106d9baa662dc9c45cd138d3d95e54a0f2f791dfb890dc91ac1c34ec80a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
